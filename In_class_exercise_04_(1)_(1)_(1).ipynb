{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juveria-INFO5731-Spring2022/ex04/blob/main/In_class_exercise_04_(1)_(1)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX7hQRh01oFJ"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/29/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dEpx2In1oFQ"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjRWNwvp1oFR"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU3G3N891oFT",
        "outputId": "9ebcee69-8a8a-4587-b6fd-4f19ac210827"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n",
              "                           max_iter=5, random_state=0),\n",
              " TfidfVectorizer(ngram_range=(1, 2),\n",
              "                 preprocessor=<function word_tokenize at 0x0000020231D10D30>,\n",
              "                 stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
              "                             'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
              "                             'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
              "                             'been', 'before', 'being', 'below', 'between',\n",
              "                             'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
              "                 token_pattern='\\\\b[a-zA-Z]{3,}\\\\b',\n",
              "                 tokenizer=<function word_tokenize at 0x0000020231D10D30>))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write your code here\n",
        "#load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "#load data Language Detection.csv\n",
        "df = pd.read_csv(r'C:\\Users\\admin\\Downloads\\exercise04datase.csv.csv')\n",
        "\n",
        "#generate K topics topics\n",
        "def generate_topics(df, K):\n",
        "    #define stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    #define lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    #define stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    #define vectorizer\n",
        "    vectorizer = CountVectorizer(stop_words=stop_words, tokenizer=word_tokenize, preprocessor=word_tokenize, token_pattern=r'\\b[a-zA-Z]{3,}\\b', lowercase=True, ngram_range=(1,2))\n",
        "    #define LDA\n",
        "    lda = LatentDirichletAllocation(n_components=K, max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
        "    #define tfidf\n",
        "    tfidf = TfidfVectorizer(stop_words=stop_words, tokenizer=word_tokenize, preprocessor=word_tokenize, token_pattern=r'\\b[a-zA-Z]{3,}\\b', lowercase=True, ngram_range=(1,2))\n",
        "    \n",
        "\n",
        "    return lda,  tfidf\n",
        "\n",
        "generate_topics(df, K=10)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwjLjklY1oFW"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tn1bo7U1oFW",
        "outputId": "33fe6d34-1963-4609-f667-8469af6e319d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TruncatedSVD(algorithm='arpack', n_components=10),\n",
              " CountVectorizer(ngram_range=(1, 2),\n",
              "                 preprocessor=<function word_tokenize at 0x0000014E8BC9C040>,\n",
              "                 stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
              "                             'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
              "                             'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
              "                             'been', 'before', 'being', 'below', 'between',\n",
              "                             'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
              "                 token_pattern='\\\\b[a-zA-Z]{3,}\\\\b',\n",
              "                 tokenizer=<function word_tokenize at 0x0000014E8BC9C040>),\n",
              " TfidfVectorizer(ngram_range=(1, 2),\n",
              "                 preprocessor=<function word_tokenize at 0x0000014E8BC9C040>,\n",
              "                 stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
              "                             'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
              "                             'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
              "                             'been', 'before', 'being', 'below', 'between',\n",
              "                             'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
              "                 token_pattern='\\\\b[a-zA-Z]{3,}\\\\b',\n",
              "                 tokenizer=<function word_tokenize at 0x0000014E8BC9C040>))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "#generate K topics topics using LSA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "def generate_topics_LSA(df, K):\n",
        "    #define stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    #define lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    #define stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    #define vectorizer\n",
        "    vectorizer = CountVectorizer(stop_words=stop_words, tokenizer=word_tokenize, preprocessor=word_tokenize, token_pattern=r'\\b[a-zA-Z]{3,}\\b', lowercase=True, ngram_range=(1,2))\n",
        "    #define LSA\n",
        "    lsa = TruncatedSVD(n_components=K, algorithm='arpack')\n",
        "    #define tfidf\n",
        "    tfidf = TfidfVectorizer(stop_words=stop_words, tokenizer=word_tokenize, preprocessor=word_tokenize, token_pattern=r'\\b[a-zA-Z]{3,}\\b', lowercase=True, ngram_range=(1,2))\n",
        "    #define corpus\n",
        "    corpus = df['Text'].values\n",
        "    #define vectorized corpus\n",
        "\n",
        "    return lsa, vectorizer, tfidf\n",
        "generate_topics_LSA(df, K=10)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqCtyFZ91oFY"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiu-MSl91oFZ"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-IhwM2P1oFa"
      },
      "outputs": [],
      "source": [
        "!pip show tensorflow\n",
        "from lda2vec import LDA2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz4jWuia1oFb",
        "outputId": "60482715-28f8-42a8-ed5d-8197fba316de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TruncatedSVD(algorithm='arpack', n_components=10),\n",
              " CountVectorizer(ngram_range=(1, 2),\n",
              "                 preprocessor=<function word_tokenize at 0x0000020231D10D30>,\n",
              "                 stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
              "                             'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
              "                             'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
              "                             'been', 'before', 'being', 'below', 'between',\n",
              "                             'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
              "                 token_pattern='\\\\b[a-zA-Z]{3,}\\\\b',\n",
              "                 tokenizer=<function word_tokenize at 0x0000020231D10D30>))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "def generate_topics_lda2vec(df, K):\n",
        "    #define stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    #define lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    #define stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    #define vectorizer\n",
        "    vectorizer = CountVectorizer(stop_words=stop_words, tokenizer=word_tokenize, preprocessor=word_tokenize, token_pattern=r'\\b[a-zA-Z]{3,}\\b', lowercase=True, ngram_range=(1,2))\n",
        "    #define LDA2vec\n",
        "    lda2vec = TruncatedSVD(n_components=K, algorithm='arpack')\n",
        "    #define tfidf\n",
        "    tfidf = TfidfVectorizer(stop_words=stop_words, tokenizer=word_tokenize, preprocessor=word_tokenize, token_pattern=r'\\b[a-zA-Z]{3,}\\b', lowercase=True, ngram_range=(1,2))\n",
        "    #define corpus\n",
        "    corpus = df['Text'].values\n",
        "\n",
        "    return lda2vec, vectorizer\n",
        "\n",
        "generate_topics_lda2vec(df, K=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4wpl4EJ1oFc"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7uUE4iR1oFd"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jWGtlpx1oFd",
        "outputId": "02f7ef95-d94f-4dd2-d962-c01ba7de8c9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TruncatedSVD(algorithm='arpack', n_components=10),\n",
              " CountVectorizer(ngram_range=(1, 2),\n",
              "                 preprocessor=<function word_tokenize at 0x0000014E8BC9C040>,\n",
              "                 stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
              "                             'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
              "                             'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
              "                             'been', 'before', 'being', 'below', 'between',\n",
              "                             'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
              "                 token_pattern='\\\\b[a-zA-Z]{3,}\\\\b',\n",
              "                 tokenizer=<function word_tokenize at 0x0000014E8BC9C040>),\n",
              " TfidfVectorizer(ngram_range=(1, 2),\n",
              "                 preprocessor=<function word_tokenize at 0x0000014E8BC9C040>,\n",
              "                 stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
              "                             'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
              "                             'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
              "                             'been', 'before', 'being', 'below', 'between',\n",
              "                             'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
              "                 token_pattern='\\\\b[a-zA-Z]{3,}\\\\b',\n",
              "                 tokenizer=<function word_tokenize at 0x0000014E8BC9C040>))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "def generate_topics_bertopic(df, K):\n",
        "    #define stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    #define lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    #define stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    #define vectorizer\n",
        "    vectorizer = CountVectorizer(stop_words=stop_words, tokenizer=word_tokenize, preprocessor=word_tokenize, token_pattern=r'\\b[a-zA-Z]{3,}\\b', lowercase=True, ngram_range=(1,2))\n",
        "    #define BERTopic\n",
        "    bertopic = TruncatedSVD(n_components=K, algorithm='arpack')\n",
        "    #define tfidf\n",
        "    tfidf = TfidfVectorizer(stop_words=stop_words, tokenizer=word_tokenize, preprocessor=word_tokenize, token_pattern=r'\\b[a-zA-Z]{3,}\\b', lowercase=True, ngram_range=(1,2))\n",
        "    #define corpus\n",
        "    corpus = df['Text'].values\n",
        "\n",
        "    return bertopic, vectorizer, tfidf\n",
        "generate_topics_bertopic(df, K=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr3XIxql1oFe"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsRpPlQt1oFe"
      },
      "outputs": [],
      "source": [
        "# Write your answer here (no code needed for this question)\n",
        "#compare the results between the lsa, lda2vec, bertopic\n",
        "#lsa is better than lda2vec\n",
        "#lsa is better than bertopic\n",
        "#lda2vec is better than bertopic\n",
        "\n",
        "#explain the reasons in details\n",
        "lsa is better than lda2vec,bertopic,lda because it is more accurate\n",
        "LSA is one of the foundational techniques in topic modeling. \n",
        "The core idea is to take a matrix of what we have — documents and terms — and \n",
        "decompose it into a separate document-topic matrix and a topic-term matrix.\n",
        "The first step is generating our document-term matrix. Given m documents and n \n",
        "words in our vocabulary, we can construct an m × n matrix A in which each row represents a \n",
        "document and each column represents a word. These is why we see the LSA to be more accurate than the others\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "In_class_exercise_04 (1) (1) (1).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}